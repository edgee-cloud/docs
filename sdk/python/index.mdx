---
title: Python SDK
sidebarTitle: Python
description: Integrate the Python SDK in your application.
icon: python
---

The Edgee Python SDK provides a lightweight, type-safe interface to interact with the Edgee AI Gateway. It supports OpenAI-compatible chat completions, function calling, and streaming.

## Installation

```bash
pip install edgee
```

## Quick Start

```python
from edgee import Edgee

edgee = Edgee()

response = edgee.send(
    model="gpt-4o",
    input="What is the capital of France?"
)

print(response.text)
# "The capital of France is Paris."
```

## Configuration

The SDK can be configured in multiple ways:

### Using Environment Variables

```python
import os

# Set EDGEE_API_KEY environment variable
edgee = Edgee()
```

### Using Constructor Parameters

```python
# String API key (backward compatible)
edgee = Edgee("your-api-key")

# Configuration object
from edgee import EdgeeConfig

edgee = Edgee(EdgeeConfig(
    api_key="your-api-key",
    base_url="https://api.edgee.ai"  # optional, defaults to https://api.edgee.ai
))

# Dictionary configuration
edgee = Edgee({
    "api_key": "your-api-key",
    "base_url": "https://api.edgee.ai"
})
```

## Usage Examples

### Simple String Input

The simplest way to send a request is with a string input:

```python
response = edgee.send(
    model="gpt-4o",
    input="Explain quantum computing in simple terms."
)

print(response.text)
```

### Full Message Array

For more control, use a full message array:

```python
response = edgee.send(
    model="gpt-4o",
    input={
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]
    }
)

print(response.text)
```

### Function Calling (Tools)

The SDK supports OpenAI-compatible function calling:

```python
response = edgee.send(
    model="gpt-4o",
    input={
        "messages": [
            {"role": "user", "content": "What is the weather in Paris?"}
        ],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get the current weather for a location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "City name"
                            }
                        },
                        "required": ["location"]
                    }
                }
            }
        ],
        "tool_choice": "auto"  # or "none", or {"type": "function", "function": {"name": "get_weather"}}
    }
)

# Check if the model wants to call a function
if response.tool_calls:
    tool_call = response.tool_calls[0]
    print(f"Function: {tool_call['function']['name']}")
    print(f"Arguments: {tool_call['function']['arguments']}")
```

### Tool Response Handling

After receiving a tool call, you can send the function result back:

```python
import json

# First request - model requests a tool call
response1 = edgee.send(
    model="gpt-4o",
    input={
        "messages": [{"role": "user", "content": "What is the weather in Paris?"}],
        "tools": [...]  # tool definitions
    }
)

# Execute the function and send the result
tool_call = response1.tool_calls[0]
function_result = get_weather(json.loads(tool_call['function']['arguments']))

# Second request - include tool response
response2 = edgee.send(
    model="gpt-4o",
    input={
        "messages": [
            {"role": "user", "content": "What is the weather in Paris?"},
            response1.message,  # Include the assistant's message with tool_calls
            {
                "role": "tool",
                "tool_call_id": tool_call['id'],
                "content": json.dumps(function_result)
            }
        ]
    }
)

print(response2.text)
```

## Streaming

The SDK supports streaming responses for real-time output. Use streaming when you want to display tokens as they're generated.

### Simple Text Streaming

The easiest way to stream is using `stream_text()`, which yields only the text content:

```python
# Stream text only
for text in edgee.stream_text("gpt-4o", "Tell me a story"):
    print(text, end="", flush=True)
```

### Full Chunk Streaming

For more control, use `stream()` to access full chunk metadata:

```python
# Stream full chunks with metadata
for chunk in edgee.stream("gpt-4o", "Explain quantum computing"):
    # First chunk contains the role
    if chunk.role:
        print(f"Role: {chunk.role}")

    # Content chunks
    if chunk.text:
        print(chunk.text, end="", flush=True)

    # Last chunk contains finish reason
    if chunk.finish_reason:
        print(f"\nFinish reason: {chunk.finish_reason}")
```

### Streaming with Messages

Streaming works with full message arrays too:

```python
for chunk in edgee.stream(
    "gpt-4o",
    {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Write a poem about coding"}
        ]
    }
):
    if chunk.text:
        print(chunk.text, end="", flush=True)
```

### Using send() with stream Parameter

You can also use the `send()` method with `stream=True`:

```python
# Returns a generator instead of SendResponse
for chunk in edgee.send("gpt-4o", "Tell me a story", stream=True):
    if chunk.text:
        print(chunk.text, end="", flush=True)
```

### Streaming Response Types

Streaming uses different response types:

```python
# StreamChunk - returned by stream()
@dataclass
class StreamChunk:
    choices: list[StreamChoice]

    # Convenience properties
    text: str | None         # Get content from first choice
    role: str | None         # Get role from first choice
    finish_reason: str | None # Get finish_reason from first choice

@dataclass
class StreamChoice:
    index: int
    delta: StreamDelta
    finish_reason: str | None

@dataclass
class StreamDelta:
    role: str | None = None
    content: str | None = None
    tool_calls: list[dict] | None = None
```

### Convenience Properties

Both `SendResponse` and `StreamChunk` have convenience properties for easier access:

```python
# Non-streaming response
response = edgee.send("gpt-4o", "Hello")
print(response.text)          # Instead of response.choices[0].message["content"]
print(response.finish_reason) # Instead of response.choices[0].finish_reason
print(response.tool_calls)    # Instead of response.choices[0].message.get("tool_calls")

# Streaming response
for chunk in edgee.stream("gpt-4o", "Hello"):
    print(chunk.text)          # Instead of chunk.choices[0].delta.content
    print(chunk.role)          # Instead of chunk.choices[0].delta.role
    print(chunk.finish_reason) # Instead of chunk.choices[0].finish_reason
```

## Response Structure

The `send` method returns a `SendResponse` object:

```python
@dataclass
class SendResponse:
    choices: list[Choice]
    usage: Usage | None = None

@dataclass
class Choice:
    index: int
    message: dict  # {"role": str, "content": str, "tool_calls": list | None}
    finish_reason: str | None

@dataclass
class Usage:
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
```

### Accessing Response Data

```python
response = edgee.send("gpt-4o", "Hello!")

# Get the first choice's content
content = response.text

# Check finish reason
finish_reason = response.finish_reason  # 'stop', 'length', 'tool_calls', etc.

# Access token usage
if response.usage:
    print(f"Tokens used: {response.usage.total_tokens}")
    print(f"Prompt tokens: {response.usage.prompt_tokens}")
    print(f"Completion tokens: {response.usage.completion_tokens}")
```

## Type Definitions

The SDK exports Python dataclasses for all request and response objects:

```python
from edgee import (
    Edgee,
    EdgeeConfig,
    Message,
    Tool,
    FunctionDefinition,
    ToolCall,
    InputObject,
    SendResponse,
    StreamChunk,
    Choice,
    Usage
)
```

### Message Types

```python
@dataclass
class Message:
    role: str  # "system" | "user" | "assistant" | "tool"
    content: str | None = None
    name: str | None = None
    tool_calls: list[ToolCall] | None = None
    tool_call_id: str | None = None
```

### Tool Types

```python
@dataclass
class FunctionDefinition:
    name: str
    description: str | None = None
    parameters: dict | None = None

@dataclass
class Tool:
    type: str  # "function"
    function: FunctionDefinition

@dataclass
class ToolCall:
    id: str
    type: str
    function: dict  # {"name": str, "arguments": str}
```

## Error Handling

The SDK raises exceptions for common issues:

```python
from edgee import Edgee

try:
    edgee = Edgee()  # Raises ValueError if EDGEE_API_KEY is not set
except ValueError as error:
    print(f"Configuration error: {error}")

try:
    response = edgee.send("gpt-4o", "Hello!")
except RuntimeError as error:
    print(f"Request failed: {error}")
    # Handle API errors, network errors, etc.
```

## What's Next?

<CardGroup cols={2}>
  <Card title="API Reference" icon="terminal" href="/api-reference">
    Explore the full REST API documentation.
  </Card>
  <Card title="Supported Models" icon="brain" href="https://www.edgee.cloud/models">
    Browse 200+ models available through Edgee.
  </Card>
  <Card title="Features" icon="sparkles" href="/features">
    Learn about intelligent routing, observability, and privacy controls.
  </Card>
  <Card title="Quickstart Guide" icon="rocket" href="/quickstart">
    Get started with Edgee in minutes.
  </Card>
</CardGroup>
